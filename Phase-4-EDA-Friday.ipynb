{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "1b740ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "\n",
    "import string\n",
    "import re\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "34d54494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('phase-4-dataset.csv', encoding= 'latin1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cacbb14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9093, 3)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187864d3",
   "metadata": {},
   "source": [
    "Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "bb4e5e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={\"is_there_an_emotion_directed_at_a_brand_or_product\" : \"emotion\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752198fd",
   "metadata": {},
   "source": [
    "Adding a Label Encoder to our Target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "77734358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>emotion</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at           emotion  label  \n",
       "0                          iPhone  Negative emotion      1  \n",
       "1              iPad or iPhone App  Positive emotion      3  \n",
       "2                            iPad  Positive emotion      3  \n",
       "3              iPad or iPhone App  Negative emotion      1  \n",
       "4                          Google  Positive emotion      3  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(df['emotion'])\n",
    "df['label'] = le.transform(df['emotion'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "e880d96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = dict(zip(le.classes_, range(len(le.classes_))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ae21a1",
   "metadata": {},
   "source": [
    "Seeing how many labels we're working with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "d35bb29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['label']%2==0].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "55620c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae9e0df",
   "metadata": {},
   "source": [
    " any nulls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4e4065e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "357"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9b99f844",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3548 entries, 0 to 9088\n",
      "Data columns (total 4 columns):\n",
      " #   Column                           Non-Null Count  Dtype \n",
      "---  ------                           --------------  ----- \n",
      " 0   tweet_text                       3548 non-null   object\n",
      " 1   emotion_in_tweet_is_directed_at  3191 non-null   object\n",
      " 2   emotion                          3548 non-null   object\n",
      " 3   label                            3548 non-null   int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 138.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb66c2",
   "metadata": {},
   "source": [
    "### taking a look at a random tweet from the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a132fb",
   "metadata": {},
   "source": [
    "### Let's see how it compares to one further down the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "63e31ac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@mention check it. RT @mention #SXSW FREE App Festival Explorer: find the bands you want to see from your music tastes {link}'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[2345].tweet_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf40dab2",
   "metadata": {},
   "source": [
    "### What is the target for tweet 2345?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "84989424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive emotion'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[2345].emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf20708a",
   "metadata": {},
   "source": [
    "### Let's look at another random tweet followed by its target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c2e38495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @mention RT @mention Google to Launch Major New Social Network Called Circles, {link} #sxsw #nptech'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[6543].tweet_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befefddb",
   "metadata": {},
   "source": [
    "It looks like whenever someone types the \"@\" symbol it is translated to the phrase \"@mention\" instead of \"@-the-person-or-company.\" Probably this is to protect the public so we can use this corpus and its documents freely. We'll take care of that after Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e98a891e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive emotion'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[6543].emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374f4614",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e005fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('label', axis=1)\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76cae3f",
   "metadata": {},
   "source": [
    "## Functions that strip mentions, retweets, and embedded things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8ba2cbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_junk(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9#]+', '', str(text)) #remove @mentions\n",
    "    text = re.sub(r'RT[\\s]+', '', str(text)) # remove RT\n",
    "    text = re.sub(r'\\[VIDEO\\]', '', str(text)) # remove [VIDEO] describer\n",
    "    text = re.sub(r'\\{link\\}', '', str(text)) # remove {link} describer\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text) # removes actual links\n",
    "    text = re.sub(r'[\\,\\.\\?\\*\\$\\'\\!\\(\\)\\:\\_\\/\\-\\=\\^\\;]+', '', text)\n",
    "    return text\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ae8e6229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to remove all non-ASCII characters \n",
    "def remove_nonASCII(x):\n",
    "    text = ''.join([c for c in x if ord(c) < 128])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "64e0c3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "\n",
    "def tokenize_sw(tweet, stop_words=sw):\n",
    "    '''\n",
    "    \n",
    "    :param doc: a document from the satire corpus \n",
    "    :return: a document string with words which have been \n",
    "            lemmatized, \n",
    "            parsed for stopwords, \n",
    "            made lowercase,\n",
    "            and stripped of punctuation and numbers.\n",
    "    '''\n",
    "    \n",
    "    regex_token = RegexpTokenizer(r\"([a-zA-Z]+(?:’[a-z]+)?)\")\n",
    "    tokenized_tweet = regex_token.tokenize(tweet)\n",
    "    lowered_tokens = [word.lower() for word in tokenized_tweet]\n",
    "    cleaned_tweet = [word for word in lowered_tokens if word not in sw]\n",
    "    return cleaned_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a5ccb57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['tweet_text'] = X_train['tweet_text'].apply(remove_junk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "67f81ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['clean_tweet'] = X_train['tweet_text'].apply(remove_nonASCII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0242e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['clean_tokens'] = X_train['clean_tweet'].apply(tokenize_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "90d5d06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'friends': 1, 'austin': 1, 'sxsw': 1, 'ill': 1, 'buffalo': 1, 'new': 1, 'ipad': 1})"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "example_freq_dist = FreqDist(X_train.iloc[100][\"clean_tokens\"][:20])\n",
    "example_freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bc230e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def visualize_top_10(freq_dist, title):\n",
    "\n",
    "    # Extract data for plotting\n",
    "    top_10 = list(zip(*freq_dist.most_common(10)))\n",
    "    tokens = top_10[0]\n",
    "    counts = top_10[1]\n",
    "\n",
    "    # Set up plot and plot data\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(tokens, counts)\n",
    "\n",
    "    # Customize plot appearance\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.tick_params(axis=\"x\", rotation=90)\n",
    "    \n",
    "train_freq_dist = FreqDist(X_train['clean_tokens'].explode())\n",
    "\n",
    "# Plot the top 10 tokens\n",
    "visualize_top_10(train_freq_dist, \"Top 10 Word Frequency for 5 trains\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5362692c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>emotion</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>clean_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>the first iPad didnt even exist here last year...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>the first iPad didnt even exist here last year...</td>\n",
       "      <td>[first, ipad, didnt, even, exist, last, year, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3060</th>\n",
       "      <td>Thats awesome Its not a rumor Apple is opening...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Thats awesome Its not a rumor Apple is opening...</td>\n",
       "      <td>[thats, awesome, rumor, apple, opening, tempor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088</th>\n",
       "      <td>had to charge the #iphone every 6hours here on...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>had to charge the #iphone every 6hours here on...</td>\n",
       "      <td>[charge, iphone, every, hours, sxsw, running, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Really COOL Stop by and let us make you a cust...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Really COOL Stop by and let us make you a cust...</td>\n",
       "      <td>[really, cool, stop, let, us, make, custom, ip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5931</th>\n",
       "      <td>Googles Marissa Mayer on the locationbased fas...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Googles Marissa Mayer on the locationbased fas...</td>\n",
       "      <td>[googles, marissa, mayer, locationbased, fast,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7253</th>\n",
       "      <td>Banks innovate or die is a great session &amp;quot...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Banks innovate or die is a great session &amp;quot...</td>\n",
       "      <td>[banks, innovate, die, great, session, quot, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4833</th>\n",
       "      <td>Heading to Austin for #SXSW The  Austin guide ...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Heading to Austin for #SXSW The  Austin guide ...</td>\n",
       "      <td>[heading, austin, sxsw, austin, guide, iphone,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3568</th>\n",
       "      <td>I used to think that and then they started ma...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>I used to think that and then they started ma...</td>\n",
       "      <td>[used, think, started, making, great, apps, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5278</th>\n",
       "      <td>#SXSW #sxswparty Matt Damon upstairs at the Go...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>#SXSW #sxswparty Matt Damon upstairs at the Go...</td>\n",
       "      <td>[sxsw, sxswparty, matt, damon, upstairs, googl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6611</th>\n",
       "      <td>The day Bank of America launched their iPhone ...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>The day Bank of America launched their iPhone ...</td>\n",
       "      <td>[day, bank, america, launched, iphone, app, go...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2661 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text  \\\n",
       "692   the first iPad didnt even exist here last year...   \n",
       "3060  Thats awesome Its not a rumor Apple is opening...   \n",
       "2088  had to charge the #iphone every 6hours here on...   \n",
       "4998  Really COOL Stop by and let us make you a cust...   \n",
       "5931  Googles Marissa Mayer on the locationbased fas...   \n",
       "...                                                 ...   \n",
       "7253  Banks innovate or die is a great session &quot...   \n",
       "4833  Heading to Austin for #SXSW The  Austin guide ...   \n",
       "3568   I used to think that and then they started ma...   \n",
       "5278  #SXSW #sxswparty Matt Damon upstairs at the Go...   \n",
       "6611  The day Bank of America launched their iPhone ...   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at           emotion  \\\n",
       "692                             iPad  Negative emotion   \n",
       "3060                           Apple  Positive emotion   \n",
       "2088                          iPhone  Negative emotion   \n",
       "4998                          iPhone  Positive emotion   \n",
       "5931                          Google  Positive emotion   \n",
       "...                              ...               ...   \n",
       "7253              iPad or iPhone App  Positive emotion   \n",
       "4833              iPad or iPhone App  Positive emotion   \n",
       "3568              iPad or iPhone App  Positive emotion   \n",
       "5278                             NaN  Positive emotion   \n",
       "6611              iPad or iPhone App  Positive emotion   \n",
       "\n",
       "                                            clean_tweet  \\\n",
       "692   the first iPad didnt even exist here last year...   \n",
       "3060  Thats awesome Its not a rumor Apple is opening...   \n",
       "2088  had to charge the #iphone every 6hours here on...   \n",
       "4998  Really COOL Stop by and let us make you a cust...   \n",
       "5931  Googles Marissa Mayer on the locationbased fas...   \n",
       "...                                                 ...   \n",
       "7253  Banks innovate or die is a great session &quot...   \n",
       "4833  Heading to Austin for #SXSW The  Austin guide ...   \n",
       "3568   I used to think that and then they started ma...   \n",
       "5278  #SXSW #sxswparty Matt Damon upstairs at the Go...   \n",
       "6611  The day Bank of America launched their iPhone ...   \n",
       "\n",
       "                                           clean_tokens  \n",
       "692   [first, ipad, didnt, even, exist, last, year, ...  \n",
       "3060  [thats, awesome, rumor, apple, opening, tempor...  \n",
       "2088  [charge, iphone, every, hours, sxsw, running, ...  \n",
       "4998  [really, cool, stop, let, us, make, custom, ip...  \n",
       "5931  [googles, marissa, mayer, locationbased, fast,...  \n",
       "...                                                 ...  \n",
       "7253  [banks, innovate, die, great, session, quot, k...  \n",
       "4833  [heading, austin, sxsw, austin, guide, iphone,...  \n",
       "3568  [used, think, started, making, great, apps, of...  \n",
       "5278  [sxsw, sxswparty, matt, damon, upstairs, googl...  \n",
       "6611  [day, bank, america, launched, iphone, app, go...  \n",
       "\n",
       "[2661 rows x 5 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "658125ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amp</th>\n",
       "      <th>android</th>\n",
       "      <th>app</th>\n",
       "      <th>apple</th>\n",
       "      <th>austin</th>\n",
       "      <th>circles</th>\n",
       "      <th>free</th>\n",
       "      <th>get</th>\n",
       "      <th>google</th>\n",
       "      <th>great</th>\n",
       "      <th>...</th>\n",
       "      <th>line</th>\n",
       "      <th>new</th>\n",
       "      <th>popup</th>\n",
       "      <th>quot</th>\n",
       "      <th>social</th>\n",
       "      <th>store</th>\n",
       "      <th>sxsw</th>\n",
       "      <th>time</th>\n",
       "      <th>today</th>\n",
       "      <th>via</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.140134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.468229</td>\n",
       "      <td>0.668722</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.544882</td>\n",
       "      <td>0.191510</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.652167</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.266743</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351868</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2656</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.451512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.603205</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.509136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.723108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454035</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103543</td>\n",
       "      <td>0.429389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.544337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.727217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.931379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.364052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2660</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.584887</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.60622</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.189619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2661 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      amp  android       app     apple    austin  circles      free  get  \\\n",
       "0     0.0      0.0  0.000000  0.000000  0.000000      0.0  0.000000  0.0   \n",
       "1     0.0      0.0  0.000000  0.468229  0.668722      0.0  0.000000  0.0   \n",
       "2     0.0      0.0  0.000000  0.652167  0.000000      0.0  0.000000  0.0   \n",
       "3     0.0      0.0  0.000000  0.000000  0.000000      0.0  0.000000  0.0   \n",
       "4     0.0      0.0  0.000000  0.000000  0.000000      0.0  0.000000  0.0   \n",
       "...   ...      ...       ...       ...       ...      ...       ...  ...   \n",
       "2656  0.0      0.0  0.451512  0.000000  0.000000      0.0  0.000000  0.0   \n",
       "2657  0.0      0.0  0.000000  0.000000  0.723108      0.0  0.454035  0.0   \n",
       "2658  0.0      0.0  0.544337  0.000000  0.000000      0.0  0.000000  0.0   \n",
       "2659  0.0      0.0  0.000000  0.000000  0.000000      0.0  0.000000  0.0   \n",
       "2660  0.0      0.0  0.584887  0.000000  0.000000      0.0  0.000000  0.0   \n",
       "\n",
       "        google     great  ...  line      new  popup      quot  social  \\\n",
       "0     0.000000  0.000000  ...   0.0  0.00000    0.0  0.000000     0.0   \n",
       "1     0.000000  0.000000  ...   0.0  0.00000    0.0  0.000000     0.0   \n",
       "2     0.000000  0.000000  ...   0.0  0.00000    0.0  0.000000     0.0   \n",
       "3     0.000000  0.000000  ...   0.0  0.00000    0.0  0.000000     0.0   \n",
       "4     0.000000  0.000000  ...   0.0  0.00000    0.0  0.000000     0.0   \n",
       "...        ...       ...  ...   ...      ...    ...       ...     ...   \n",
       "2656  0.000000  0.603205  ...   0.0  0.00000    0.0  0.509136     0.0   \n",
       "2657  0.000000  0.000000  ...   0.0  0.00000    0.0  0.000000     0.0   \n",
       "2658  0.000000  0.727217  ...   0.0  0.00000    0.0  0.000000     0.0   \n",
       "2659  0.931379  0.000000  ...   0.0  0.00000    0.0  0.000000     0.0   \n",
       "2660  0.000000  0.000000  ...   0.0  0.60622    0.0  0.000000     0.0   \n",
       "\n",
       "         store      sxsw      time  today  via  \n",
       "0     0.000000  0.140134  0.000000    0.0  0.0  \n",
       "1     0.544882  0.191510  0.000000    0.0  0.0  \n",
       "2     0.000000  0.266743  0.000000    0.0  0.0  \n",
       "3     0.000000  0.351868  0.000000    0.0  0.0  \n",
       "4     0.000000  1.000000  0.000000    0.0  0.0  \n",
       "...        ...       ...       ...    ...  ...  \n",
       "2656  0.000000  0.146379  0.000000    0.0  0.0  \n",
       "2657  0.000000  0.103543  0.429389    0.0  0.0  \n",
       "2658  0.000000  0.176473  0.000000    0.0  0.0  \n",
       "2659  0.000000  0.364052  0.000000    0.0  0.0  \n",
       "2660  0.000000  0.189619  0.000000    0.0  0.0  \n",
       "\n",
       "[2661 rows x 25 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the relevant vectorizer class\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate a vectorizer with max_features=10\n",
    "# (we are using the default token pattern)\n",
    "tfidf = TfidfVectorizer(stop_words=sw, max_features=25)\n",
    "\n",
    "# Fit the vectorizer on X_train[\"text\"] and transform it\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['tweet_text'])\n",
    "\n",
    "# Visually inspect the 10 most common words\n",
    "pd.DataFrame.sparse.from_spmatrix(X_train_vectorized, columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12a44a6",
   "metadata": {},
   "source": [
    "## Comparing with no Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3c423067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>app</th>\n",
       "      <th>apple</th>\n",
       "      <th>at</th>\n",
       "      <th>austin</th>\n",
       "      <th>for</th>\n",
       "      <th>google</th>\n",
       "      <th>in</th>\n",
       "      <th>ipad</th>\n",
       "      <th>...</th>\n",
       "      <th>of</th>\n",
       "      <th>on</th>\n",
       "      <th>quot</th>\n",
       "      <th>store</th>\n",
       "      <th>sxsw</th>\n",
       "      <th>the</th>\n",
       "      <th>to</th>\n",
       "      <th>up</th>\n",
       "      <th>with</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.448658</td>\n",
       "      <td>0.380933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.597116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.138983</td>\n",
       "      <td>0.258406</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.33772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.482329</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.363138</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.393008</td>\n",
       "      <td>0.138131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.473165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.29876</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.717979</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122196</td>\n",
       "      <td>0.227194</td>\n",
       "      <td>0.258356</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.517878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188948</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.665921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.603884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.647280</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.220327</td>\n",
       "      <td>0.409645</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2656</th>\n",
       "      <td>0.423760</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.404908</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.346117</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.456585</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.131270</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.277542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2657</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.633109</td>\n",
       "      <td>0.643606</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090656</td>\n",
       "      <td>0.168553</td>\n",
       "      <td>0.191672</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2658</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.697666</td>\n",
       "      <td>0.392573</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.273399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.373901</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.127271</td>\n",
       "      <td>0.236631</td>\n",
       "      <td>0.269087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2659</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.548934</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.644706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.251999</td>\n",
       "      <td>0.468532</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2660</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498928</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426486</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.161752</td>\n",
       "      <td>0.300738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2661 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            an       and       app    apple        at    austin       for  \\\n",
       "0     0.448658  0.380933  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
       "1     0.000000  0.000000  0.000000  0.33772  0.000000  0.482329  0.000000   \n",
       "2     0.000000  0.000000  0.000000  0.29876  0.000000  0.000000  0.000000   \n",
       "3     0.000000  0.517878  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
       "4     0.000000  0.603884  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
       "...        ...       ...       ...      ...       ...       ...       ...   \n",
       "2656  0.423760  0.000000  0.404908  0.00000  0.000000  0.000000  0.000000   \n",
       "2657  0.000000  0.000000  0.000000  0.00000  0.000000  0.633109  0.643606   \n",
       "2658  0.000000  0.697666  0.392573  0.00000  0.000000  0.000000  0.000000   \n",
       "2659  0.000000  0.000000  0.000000  0.00000  0.548934  0.000000  0.000000   \n",
       "2660  0.000000  0.000000  0.498928  0.00000  0.000000  0.000000  0.000000   \n",
       "\n",
       "        google        in      ipad  ...        of        on      quot  \\\n",
       "0     0.000000  0.000000  0.597116  ...  0.000000  0.000000  0.000000   \n",
       "1     0.000000  0.363138  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2     0.000000  0.000000  0.000000  ...  0.000000  0.717979  0.000000   \n",
       "3     0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "4     0.000000  0.000000  0.000000  ...  0.000000  0.647280  0.000000   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2656  0.000000  0.000000  0.000000  ...  0.346117  0.000000  0.456585   \n",
       "2657  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2658  0.000000  0.000000  0.273399  ...  0.000000  0.373901  0.000000   \n",
       "2659  0.644706  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "2660  0.000000  0.000000  0.000000  ...  0.426486  0.000000  0.000000   \n",
       "\n",
       "         store      sxsw       the        to        up  with       you  \n",
       "0     0.000000  0.138983  0.258406  0.000000  0.000000   0.0  0.000000  \n",
       "1     0.393008  0.138131  0.000000  0.000000  0.473165   0.0  0.000000  \n",
       "2     0.000000  0.122196  0.227194  0.258356  0.000000   0.0  0.000000  \n",
       "3     0.000000  0.188948  0.000000  0.000000  0.000000   0.0  0.665921  \n",
       "4     0.000000  0.220327  0.409645  0.000000  0.000000   0.0  0.000000  \n",
       "...        ...       ...       ...       ...       ...   ...       ...  \n",
       "2656  0.000000  0.131270  0.000000  0.277542  0.000000   0.0  0.000000  \n",
       "2657  0.000000  0.090656  0.168553  0.191672  0.000000   0.0  0.000000  \n",
       "2658  0.000000  0.127271  0.236631  0.269087  0.000000   0.0  0.000000  \n",
       "2659  0.000000  0.251999  0.468532  0.000000  0.000000   0.0  0.000000  \n",
       "2660  0.000000  0.161752  0.300738  0.000000  0.000000   0.0  0.000000  \n",
       "\n",
       "[2661 rows x 25 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the relevant vectorizer class\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Instantiate a vectorizer with max_features=10\n",
    "# (we are using the default token pattern)\n",
    "tfidf = TfidfVectorizer(max_features=25)\n",
    "\n",
    "# Fit the vectorizer on X_train[\"text\"] and transform it\n",
    "X_train_vectorized = tfidf.fit_transform(X_train['tweet_text'])\n",
    "\n",
    "# Visually inspect the 10 most common words\n",
    "pd.DataFrame.sparse.from_spmatrix(X_train_vectorized, columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab029c53",
   "metadata": {},
   "source": [
    "## Greg's Lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4fb1ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    '''\n",
    "    Translate nltk POS to wordnet tags\n",
    "    '''\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "fbcd59bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_preparer(doc, stop_words=sw):\n",
    "    '''\n",
    "    \n",
    "    :param doc: a document from the satire corpus \n",
    "    :return: a document string with words which have been \n",
    "            lemmatized, \n",
    "            parsed for stopwords, \n",
    "            made lowercase,\n",
    "            and stripped of punctuation and numbers.\n",
    "    '''\n",
    "    \n",
    "    regex_token = RegexpTokenizer(r\"([a-zA-Z]+(?:’[a-z]+)?)\")\n",
    "    doc = regex_token.tokenize(doc)\n",
    "    doc = [word.lower() for word in doc]\n",
    "    doc = [word for word in doc if word not in sw]\n",
    "    #print(doc)\n",
    "    doc = pos_tag(doc)\n",
    "    doc = [(word[0], get_wordnet_pos(word[1])) for word in doc]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    doc = [lemmatizer.lemmatize(word[0], word[1]) for word in doc]\n",
    "    return ' '.join(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d6358992",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_docs = [doc_preparer(doc, sw) for doc in X_train['tweet_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3c3032a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secondary train-test split to build our best model\n",
    "X_t, X_val, y_t, y_val = train_test_split(token_docs, y_train,\n",
    "                                          test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ebaa29d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features=5)\n",
    "\n",
    "X_t_vec = cv.fit_transform(X_t)\n",
    "X_t_vec = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(cv.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "d94a07b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>apple</th>\n",
       "      <th>google</th>\n",
       "      <th>ipad</th>\n",
       "      <th>iphone</th>\n",
       "      <th>sxsw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3249</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6171</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1805</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5715</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7358</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6659</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3044</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4342</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1995 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      apple  google  ipad  iphone  sxsw\n",
       "1361      0       0     1       0     1\n",
       "3249      1       0     0       0     1\n",
       "769       0       1     0       0     1\n",
       "6171      0       0     0       0     1\n",
       "1805      1       0     0       0     1\n",
       "...     ...     ...   ...     ...   ...\n",
       "5715      0       0     0       1     1\n",
       "7358      0       0     1       0     1\n",
       "6659      0       0     0       0     1\n",
       "3044      1       0     0       0     2\n",
       "4342      0       1     0       0     1\n",
       "\n",
       "[1995 rows x 5 columns]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4130f1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then transform the validation set. (Do not refit the vectorizer!)\n",
    "\n",
    "X_val_vec = cv.transform(X_val)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(cv.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "1049e7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_t_vec, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "8294c9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8355889724310777 0.1644110275689223\n"
     ]
    }
   ],
   "source": [
    "#What should our priors for each class be?\n",
    "\n",
    "prior_1 = y_t.value_counts()[1]/len(y_t)\n",
    "prior_3 = y_t.value_counts()[3]/len(y_t)\n",
    "print(prior_3, prior_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4703a15c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.80538572, -0.17961845])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb.class_log_prior_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7cf6af6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8513513513513513"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = mnb.predict(X_val_vec)\n",
    "accuracy_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9f075b",
   "metadata": {},
   "source": [
    "## Checking with 50 words vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "490e730e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8355889724310777 0.1644110275689223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8318318318318318"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(max_features=50)\n",
    "\n",
    "X_t_vec = cv.fit_transform(X_t)\n",
    "X_t_vec = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(cv.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)\n",
    "\n",
    "X_t_vec\n",
    "\n",
    "# We then transform the validation set. (Do not refit the vectorizer!)\n",
    "\n",
    "X_val_vec = cv.transform(X_val)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(cv.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_t_vec, y_t)\n",
    "\n",
    "#What should our priors for each class be?\n",
    "\n",
    "prior_1 = y_t.value_counts()[1]/len(y_t)\n",
    "prior_3 = y_t.value_counts()[3]/len(y_t)\n",
    "print(prior_3, prior_1)\n",
    "\n",
    "mnb.class_log_prior_\n",
    "\n",
    "y_hat = mnb.predict(X_val_vec)\n",
    "accuracy_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444eaf68",
   "metadata": {},
   "source": [
    "## No Restrictions  ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "ac3db10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8355889724310777 0.1644110275689223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8783783783783784"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "\n",
    "X_t_vec = cv.fit_transform(X_t)\n",
    "X_t_vec = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(cv.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)\n",
    "\n",
    "X_t_vec\n",
    "\n",
    "# We then transform the validation set. (Do not refit the vectorizer!)\n",
    "\n",
    "X_val_vec = cv.transform(X_val)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(cv.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_t_vec, y_t)\n",
    "\n",
    "#What should our priors for each class be?\n",
    "\n",
    "prior_1 = y_t.value_counts()[1]/len(y_t)\n",
    "prior_3 = y_t.value_counts()[3]/len(y_t)\n",
    "print(prior_3, prior_1)\n",
    "\n",
    "mnb.class_log_prior_\n",
    "\n",
    "y_hat = mnb.predict(X_val_vec)\n",
    "accuracy_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "bc796de0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 26,  73],\n",
       "       [  8, 559]])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d92c9",
   "metadata": {},
   "source": [
    "### With an N gram range of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "662e2d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8355889724310777 0.1644110275689223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8843843843843844"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "X_t_vec = cv.fit_transform(X_t)\n",
    "X_t_vec = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(cv.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)\n",
    "\n",
    "X_t_vec\n",
    "\n",
    "# We then transform the validation set. (Do not refit the vectorizer!)\n",
    "\n",
    "X_val_vec = cv.transform(X_val)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(cv.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_t_vec, y_t)\n",
    "\n",
    "#What should our priors for each class be?\n",
    "\n",
    "prior_1 = y_t.value_counts()[1]/len(y_t)\n",
    "prior_3 = y_t.value_counts()[3]/len(y_t)\n",
    "print(prior_3, prior_1)\n",
    "\n",
    "mnb.class_log_prior_\n",
    "\n",
    "y_hat = mnb.predict(X_val_vec)\n",
    "accuracy_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5763ccb0",
   "metadata": {},
   "source": [
    "### N gram range of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "3422003b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8355889724310777 0.1644110275689223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8843843843843844"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,3))\n",
    "\n",
    "X_t_vec = cv.fit_transform(X_t)\n",
    "X_t_vec = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(cv.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)\n",
    "\n",
    "X_t_vec\n",
    "\n",
    "# We then transform the validation set. (Do not refit the vectorizer!)\n",
    "\n",
    "X_val_vec = cv.transform(X_val)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(cv.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_t_vec, y_t)\n",
    "\n",
    "#What should our priors for each class be?\n",
    "\n",
    "prior_1 = y_t.value_counts()[1]/len(y_t)\n",
    "prior_3 = y_t.value_counts()[3]/len(y_t)\n",
    "print(prior_3, prior_1)\n",
    "\n",
    "mnb.class_log_prior_\n",
    "\n",
    "y_hat = mnb.predict(X_val_vec)\n",
    "accuracy_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e57454",
   "metadata": {},
   "source": [
    "## Limiting word percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "1e5dcef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8355889724310777 0.1644110275689223\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8513513513513513"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(min_df=0.05, max_df=0.95)\n",
    "\n",
    "X_t_vec = cv.fit_transform(X_t)\n",
    "X_t_vec = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(cv.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)\n",
    "\n",
    "X_t_vec\n",
    "\n",
    "# We then transform the validation set. (Do not refit the vectorizer!)\n",
    "\n",
    "X_val_vec = cv.transform(X_val)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(cv.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_t_vec, y_t)\n",
    "\n",
    "#What should our priors for each class be?\n",
    "\n",
    "prior_1 = y_t.value_counts()[1]/len(y_t)\n",
    "prior_3 = y_t.value_counts()[3]/len(y_t)\n",
    "print(prior_3, prior_1)\n",
    "\n",
    "mnb.class_log_prior_\n",
    "\n",
    "y_hat = mnb.predict(X_val_vec)\n",
    "accuracy_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72287dcd",
   "metadata": {},
   "source": [
    "### That was not as good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92a0abe",
   "metadata": {},
   "source": [
    "# TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d887666c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8573573573573574"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_t_vec = tfidf.fit_transform(X_t)\n",
    "X_t_vec  = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(tfidf.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)\n",
    "\n",
    "X_val_vec = tfidf.transform(X_val)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(tfidf.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_t_vec, y_t)\n",
    "y_hat = mnb.predict(X_val_vec)\n",
    "\n",
    "accuracy_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e279c7f5",
   "metadata": {},
   "source": [
    "## Limiting word amounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "f2487e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8513513513513513"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(min_df=0.05, max_df=0.95)\n",
    "X_t_vec = tfidf.fit_transform(X_t)\n",
    "X_t_vec  = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(tfidf.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)\n",
    "\n",
    "X_val_vec = tfidf.transform(X_val)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(tfidf.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_t_vec, y_t)\n",
    "y_hat = mnb.predict(X_val_vec)\n",
    "\n",
    "accuracy_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b05f51",
   "metadata": {},
   "source": [
    "### Trying with ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e83836",
   "metadata": {},
   "source": [
    "### range of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "f4d6d19d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8573573573573574"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,2))\n",
    "X_t_vec = tfidf.fit_transform(X_t)\n",
    "X_t_vec  = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(tfidf.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)\n",
    "\n",
    "X_val_vec = tfidf.transform(X_val)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(tfidf.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_t_vec, y_t)\n",
    "y_hat = mnb.predict(X_val_vec)\n",
    "\n",
    "accuracy_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3382e3",
   "metadata": {},
   "source": [
    "### range of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "1924f334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8618618618618619"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,3))\n",
    "X_t_vec = tfidf.fit_transform(X_t)\n",
    "X_t_vec  = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(tfidf.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)\n",
    "\n",
    "X_val_vec = tfidf.transform(X_val)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(tfidf.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_t_vec, y_t)\n",
    "y_hat = mnb.predict(X_val_vec)\n",
    "\n",
    "accuracy_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbaf91ea",
   "metadata": {},
   "source": [
    "### range of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "46e7b991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8633633633633634"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,4))\n",
    "X_t_vec = tfidf.fit_transform(X_t)\n",
    "X_t_vec  = pd.DataFrame.sparse.from_spmatrix(X_t_vec)\n",
    "X_t_vec.columns = sorted(tfidf.vocabulary_)\n",
    "X_t_vec.set_index(y_t.index, inplace=True)\n",
    "\n",
    "X_val_vec = tfidf.transform(X_val)\n",
    "X_val_vec  = pd.DataFrame.sparse.from_spmatrix(X_val_vec)\n",
    "X_val_vec.columns = sorted(tfidf.vocabulary_)\n",
    "X_val_vec.set_index(y_val.index, inplace=True)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "mnb.fit(X_t_vec, y_t)\n",
    "y_hat = mnb.predict(X_val_vec)\n",
    "\n",
    "accuracy_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60abce2",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "58fd179d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8513513513513513"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=1000, max_features=12, max_depth=15)\n",
    "rf.fit(X_t_vec, y_t)\n",
    "y_hat = rf.predict(X_val_vec)\n",
    "accuracy_score(y_val, y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832fa3da",
   "metadata": {},
   "source": [
    "## Random Forest with grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "4ebcdb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf2 = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "bc23c466",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth': [5,10,20],\n",
    "    'max_features': [3,11,15],\n",
    "    'min_samples_leaf': [2,8,10],\n",
    "    'min_samples_split': [3,10,11],\n",
    "    'n_estimators': [10,75, 100],\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "83fa2c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_forest = GridSearchCV(rf2, params, n_jobs=-1, verbose=3, cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4a4eb857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_forest.fit(X_t_vec, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6fb00cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid_forest.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8880728",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_forest = RandomForestClassifier(max_depth=5, max_features=3, min_samples_leaf=2,\n",
    "                       min_samples_split=3, n_estimators=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6602f3a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=5, max_features=3, min_samples_leaf=2,\n",
       "                       min_samples_split=3, n_estimators=10)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_forest.fit(X_t_vec, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "c22d317f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8355889724310777"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_forest.score(X_t_vec, y_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f395537",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cbdfb7",
   "metadata": {},
   "source": [
    "https://www.vebuso.com/2020/03/svm-hyperparameter-tuning-using-gridsearchcv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "eb8ab101",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C': [0.1, 100], # took out 1, and 10 for time sake\n",
    "              'gamma': [1, 0.001], # took out 0.1,0.01\n",
    "              'kernel': ['rbf', 'poly', 'sigmoid']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1854d5",
   "metadata": {},
   "source": [
    "after running GridSearch for an hour this error occured at the very end. \n",
    "TypeError: Argument 'X' has incorrect type (expected numpy.ndarray, got csr_matrix)\n",
    "therefore, I'm trying to vconvert X_t_vec to numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5b61a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t_vec_numpy = X_t_vec.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9c4e17d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 1.3min\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 1.2min\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 1.2min\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 1.2min\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time= 1.2min\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time= 1.1min\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time=  59.6s\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time= 1.0min\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time= 1.1min\n",
      "[CV] END ........................C=0.1, gamma=1, kernel=poly; total time= 1.1min\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=  59.6s\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=  59.6s\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=  59.3s\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time= 1.0min\n",
      "[CV] END .....................C=0.1, gamma=1, kernel=sigmoid; total time=  59.4s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=  26.9s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=  27.1s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=  27.0s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=  26.8s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=  26.8s\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=  23.0s\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=  22.9s\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=  22.7s\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=  22.8s\n",
      "[CV] END ....................C=0.1, gamma=0.001, kernel=poly; total time=  22.8s\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=  22.8s\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=  22.8s\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=  22.7s\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=  22.8s\n",
      "[CV] END .................C=0.1, gamma=0.001, kernel=sigmoid; total time=  22.7s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time= 1.2min\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time= 1.3min\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time= 1.3min\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time= 1.3min\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time= 1.3min\n",
      "[CV] END ........................C=100, gamma=1, kernel=poly; total time= 1.1min\n",
      "[CV] END ........................C=100, gamma=1, kernel=poly; total time= 1.1min\n",
      "[CV] END ........................C=100, gamma=1, kernel=poly; total time= 1.1min\n",
      "[CV] END ........................C=100, gamma=1, kernel=poly; total time= 1.1min\n",
      "[CV] END ........................C=100, gamma=1, kernel=poly; total time= 1.1min\n",
      "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time= 1.0min\n",
      "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time= 1.1min\n",
      "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time= 1.0min\n",
      "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time= 1.0min\n",
      "[CV] END .....................C=100, gamma=1, kernel=sigmoid; total time= 1.0min\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time= 1.2min\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time= 1.2min\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time= 1.2min\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time= 1.2min\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time= 1.2min\n",
      "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=  22.8s\n",
      "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=  22.8s\n",
      "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=  22.8s\n",
      "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=  22.7s\n",
      "[CV] END ....................C=100, gamma=0.001, kernel=poly; total time=  22.8s\n",
      "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=  59.1s\n",
      "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=  59.9s\n",
      "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=  59.9s\n",
      "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=  58.3s\n",
      "[CV] END .................C=100, gamma=0.001, kernel=sigmoid; total time=  57.9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [0.1, 100], 'gamma': [1, 0.001],\n",
       "                         'kernel': ['rbf', 'poly', 'sigmoid']},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svc = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)\n",
    "grid_svc.fit(X_t_vec_numpy, y_t)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f147ab5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, gamma=1, kernel='sigmoid')"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "80b636ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, gamma=1, kernel='sigmoid')"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC(C=100, gamma=1, kernel='sigmoid')\n",
    "svc.fit(X_t_vec_numpy, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "6ff82c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline       import Pipeline \n",
    "from sklearn.model_selection import cross_validate\n",
    "from imblearn.over_sampling  import RandomOverSampler\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "nb_pipe2  = Pipeline([('vect',    CountVectorizer()),\n",
    "                     ('tfidf',   TfidfTransformer()),\n",
    "                     ('sampler', RandomOverSampler('minority',random_state=42)),\n",
    "                     ('model',   MultinomialNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "a1a3c55e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>emotion</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>clean_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>the first iPad didnt even exist here last year...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>the first iPad didnt even exist here last year...</td>\n",
       "      <td>[first, ipad, didnt, even, exist, last, year, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3060</th>\n",
       "      <td>Thats awesome Its not a rumor Apple is opening...</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Thats awesome Its not a rumor Apple is opening...</td>\n",
       "      <td>[thats, awesome, rumor, apple, opening, tempor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2088</th>\n",
       "      <td>had to charge the #iphone every 6hours here on...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>had to charge the #iphone every 6hours here on...</td>\n",
       "      <td>[charge, iphone, every, hours, sxsw, running, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Really COOL Stop by and let us make you a cust...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Really COOL Stop by and let us make you a cust...</td>\n",
       "      <td>[really, cool, stop, let, us, make, custom, ip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5931</th>\n",
       "      <td>Googles Marissa Mayer on the locationbased fas...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Googles Marissa Mayer on the locationbased fas...</td>\n",
       "      <td>[googles, marissa, mayer, locationbased, fast,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7253</th>\n",
       "      <td>Banks innovate or die is a great session &amp;quot...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Banks innovate or die is a great session &amp;quot...</td>\n",
       "      <td>[banks, innovate, die, great, session, quot, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4833</th>\n",
       "      <td>Heading to Austin for #SXSW The  Austin guide ...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>Heading to Austin for #SXSW The  Austin guide ...</td>\n",
       "      <td>[heading, austin, sxsw, austin, guide, iphone,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3568</th>\n",
       "      <td>I used to think that and then they started ma...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>I used to think that and then they started ma...</td>\n",
       "      <td>[used, think, started, making, great, apps, of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5278</th>\n",
       "      <td>#SXSW #sxswparty Matt Damon upstairs at the Go...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>#SXSW #sxswparty Matt Damon upstairs at the Go...</td>\n",
       "      <td>[sxsw, sxswparty, matt, damon, upstairs, googl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6611</th>\n",
       "      <td>The day Bank of America launched their iPhone ...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>The day Bank of America launched their iPhone ...</td>\n",
       "      <td>[day, bank, america, launched, iphone, app, go...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2661 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             tweet_text  \\\n",
       "692   the first iPad didnt even exist here last year...   \n",
       "3060  Thats awesome Its not a rumor Apple is opening...   \n",
       "2088  had to charge the #iphone every 6hours here on...   \n",
       "4998  Really COOL Stop by and let us make you a cust...   \n",
       "5931  Googles Marissa Mayer on the locationbased fas...   \n",
       "...                                                 ...   \n",
       "7253  Banks innovate or die is a great session &quot...   \n",
       "4833  Heading to Austin for #SXSW The  Austin guide ...   \n",
       "3568   I used to think that and then they started ma...   \n",
       "5278  #SXSW #sxswparty Matt Damon upstairs at the Go...   \n",
       "6611  The day Bank of America launched their iPhone ...   \n",
       "\n",
       "     emotion_in_tweet_is_directed_at           emotion  \\\n",
       "692                             iPad  Negative emotion   \n",
       "3060                           Apple  Positive emotion   \n",
       "2088                          iPhone  Negative emotion   \n",
       "4998                          iPhone  Positive emotion   \n",
       "5931                          Google  Positive emotion   \n",
       "...                              ...               ...   \n",
       "7253              iPad or iPhone App  Positive emotion   \n",
       "4833              iPad or iPhone App  Positive emotion   \n",
       "3568              iPad or iPhone App  Positive emotion   \n",
       "5278                             NaN  Positive emotion   \n",
       "6611              iPad or iPhone App  Positive emotion   \n",
       "\n",
       "                                            clean_tweet  \\\n",
       "692   the first iPad didnt even exist here last year...   \n",
       "3060  Thats awesome Its not a rumor Apple is opening...   \n",
       "2088  had to charge the #iphone every 6hours here on...   \n",
       "4998  Really COOL Stop by and let us make you a cust...   \n",
       "5931  Googles Marissa Mayer on the locationbased fas...   \n",
       "...                                                 ...   \n",
       "7253  Banks innovate or die is a great session &quot...   \n",
       "4833  Heading to Austin for #SXSW The  Austin guide ...   \n",
       "3568   I used to think that and then they started ma...   \n",
       "5278  #SXSW #sxswparty Matt Damon upstairs at the Go...   \n",
       "6611  The day Bank of America launched their iPhone ...   \n",
       "\n",
       "                                           clean_tokens  \n",
       "692   [first, ipad, didnt, even, exist, last, year, ...  \n",
       "3060  [thats, awesome, rumor, apple, opening, tempor...  \n",
       "2088  [charge, iphone, every, hours, sxsw, running, ...  \n",
       "4998  [really, cool, stop, let, us, make, custom, ip...  \n",
       "5931  [googles, marissa, mayer, locationbased, fast,...  \n",
       "...                                                 ...  \n",
       "7253  [banks, innovate, die, great, session, quot, k...  \n",
       "4833  [heading, austin, sxsw, austin, guide, iphone,...  \n",
       "3568  [used, think, started, making, great, apps, of...  \n",
       "5278  [sxsw, sxswparty, matt, damon, upstairs, googl...  \n",
       "6611  [day, bank, america, launched, iphone, app, go...  \n",
       "\n",
       "[2661 rows x 5 columns]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "49772bff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
       "                ('sampler',\n",
       "                 RandomOverSampler(random_state=42,\n",
       "                                   sampling_strategy='minority')),\n",
       "                ('model', MultinomialNB())])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_pipe2.fit(X_train['tweet_text'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e8870dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9421270199173243"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_pipe2.score(X_train['tweet_text'], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "34779752",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dp/5k1wgpbj6d72lnbdgvwv16l40000gn/T/ipykernel_9356/4003525125.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mgrid_search_tune\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_pipe3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mgrid_search_tune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_x' is not defined"
     ]
    }
   ],
   "source": [
    "nb_pipe3 = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stopwords)),\n",
    "    ('clf', MultinomialNB(\n",
    "        fit_prior=True, class_prior=None)),\n",
    "])\n",
    "parameters = {\n",
    "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "    'tfidf__ngram_range': [(1, 2), (1, 3), (1, 4)],\n",
    "    'clf__estimator__alpha': (1e-2, 1e-3, 1e-4)\n",
    "}\n",
    "\n",
    "grid_search_tune = GridSearchCV(nb_pipe3, parameters, cv=2, n_jobs=2, verbose=3)\n",
    "grid_search_tune.fit(train_docs, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2840bd1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "firstEnv",
   "language": "python",
   "name": "firstenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
